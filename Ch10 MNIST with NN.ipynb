{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries and data needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up datetime and logdir for tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "\n",
    "def utc_to_local(utc_dt):\n",
    "    return utc_dt.replace(tzinfo=timezone.utc).astimezone(tz=timedelta(hours=9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST (Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-017bc9b3c101>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\HICT\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\HICT\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\HICT\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\HICT\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\HICT\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "## import MNIST data\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, Cost: 6.064\n",
      "Epoch: 0002, Cost: 1.936\n",
      "Epoch: 0003, Cost: 1.236\n",
      "Epoch: 0004, Cost: 0.966\n",
      "Epoch: 0005, Cost: 0.815\n",
      "Epoch: 0006, Cost: 0.717\n",
      "Epoch: 0007, Cost: 0.650\n",
      "Epoch: 0008, Cost: 0.598\n",
      "Epoch: 0009, Cost: 0.557\n",
      "Epoch: 0010, Cost: 0.522\n",
      "Epoch: 0011, Cost: 0.498\n",
      "Epoch: 0012, Cost: 0.472\n",
      "Epoch: 0013, Cost: 0.453\n",
      "Epoch: 0014, Cost: 0.438\n",
      "Epoch: 0015, Cost: 0.423\n",
      "Epoch: 0016, Cost: 0.409\n",
      "Epoch: 0017, Cost: 0.397\n",
      "Epoch: 0018, Cost: 0.387\n",
      "Epoch: 0019, Cost: 0.379\n",
      "Epoch: 0020, Cost: 0.371\n",
      "Epoch: 0021, Cost: 0.363\n",
      "Epoch: 0022, Cost: 0.357\n",
      "Epoch: 0023, Cost: 0.349\n",
      "Epoch: 0024, Cost: 0.345\n",
      "Epoch: 0025, Cost: 0.338\n",
      "Epoch: 0026, Cost: 0.335\n",
      "Epoch: 0027, Cost: 0.330\n",
      "Epoch: 0028, Cost: 0.326\n",
      "Epoch: 0029, Cost: 0.321\n",
      "Epoch: 0030, Cost: 0.318\n",
      "Epoch: 0031, Cost: 0.315\n",
      "Epoch: 0032, Cost: 0.311\n",
      "Epoch: 0033, Cost: 0.308\n",
      "Epoch: 0034, Cost: 0.306\n",
      "Epoch: 0035, Cost: 0.301\n",
      "Epoch: 0036, Cost: 0.300\n",
      "Epoch: 0037, Cost: 0.297\n",
      "Epoch: 0038, Cost: 0.296\n",
      "Epoch: 0039, Cost: 0.292\n",
      "Epoch: 0040, Cost: 0.289\n",
      "Epoch: 0041, Cost: 0.289\n",
      "Epoch: 0042, Cost: 0.285\n",
      "Epoch: 0043, Cost: 0.283\n",
      "Epoch: 0044, Cost: 0.283\n",
      "Epoch: 0045, Cost: 0.280\n",
      "Epoch: 0046, Cost: 0.279\n",
      "Epoch: 0047, Cost: 0.277\n",
      "Epoch: 0048, Cost: 0.275\n",
      "Epoch: 0049, Cost: 0.274\n",
      "Epoch: 0050, Cost: 0.272\n",
      "Learning is Done!\n",
      "Accuracy: 0.9177\n",
      "Label:  [8]\n",
      "Prediction:  [8]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Log Directory Setting\n",
    "\n",
    "now = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "root_logdir = \"MNIST(Softmax)\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "## Resetting Graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "## Random Seed\n",
    "tf.set_random_seed(100)\n",
    "\n",
    "## place holders and variables\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Softmax\"):\n",
    "\n",
    "    W = tf.Variable(tf.random_normal([784,10]))\n",
    "    b = tf.Variable(tf.random_normal([10]))\n",
    "    hypothesis = tf.matmul(X,W) + b\n",
    "    \n",
    "## Parameters\n",
    "\n",
    "learning_rate = 0.001    \n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "                    \n",
    "\n",
    "                    \n",
    "## Cost/Loss & Optimizer\n",
    "                    \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=hypothesis, labels=tf.stop_gradient(Y)))\n",
    "## tf.stop_gradient? v2 vs v1???\n",
    "\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "                                            \n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, axis=1)) ##tf.argmax 다시 찾아보기\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    \n",
    "## Running the actual session, training the Softmax model\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(logdir)\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost=0\n",
    "        \n",
    "        for iteration in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, summary, cost_val = sess.run([train, merged_summary, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "        \n",
    "        writer.add_summary(summary, global_step=epoch)\n",
    "\n",
    "        print(f\"Epoch: {(epoch +1):04d}, Cost: {avg_cost:.3f}\")\n",
    "        \n",
    "    print(\"Learning is Done!\")\n",
    "                      \n",
    "    print(\"Accuracy:\", sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels},))\n",
    "    \n",
    "    ##get one and predict\n",
    "                      \n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    \n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r: r+1], axis=1)))\n",
    "    print(\"Prediction: \", sess.run(tf.argmax(hypothesis, axis=1), feed_dict={X: mnist.test.images[r:r+1]}),)\n",
    "                      \n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28,28), cmap=\"Greys\", interpolation=\"nearest\",)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST (NN with normal random initializer, 3 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, Cost: 151.623\n",
      "Epoch: 0002, Cost: 38.084\n",
      "Epoch: 0003, Cost: 23.888\n",
      "Epoch: 0004, Cost: 16.628\n",
      "Epoch: 0005, Cost: 11.853\n",
      "Epoch: 0006, Cost: 8.515\n",
      "Epoch: 0007, Cost: 6.241\n",
      "Epoch: 0008, Cost: 4.571\n",
      "Epoch: 0009, Cost: 3.335\n",
      "Epoch: 0010, Cost: 2.445\n",
      "Epoch: 0011, Cost: 1.877\n",
      "Epoch: 0012, Cost: 1.380\n",
      "Epoch: 0013, Cost: 1.061\n",
      "Epoch: 0014, Cost: 0.801\n",
      "Epoch: 0015, Cost: 0.681\n",
      "Learning is Done!\n",
      "Accuracy: 0.9473\n",
      "Label:  [0]\n",
      "Prediction:  [0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Log Directoroy Setting\n",
    "\n",
    "now = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "root_logdir = \"MNIST(NN)\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "## Resetting Graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "## Random Seed\n",
    "tf.set_random_seed(100)\n",
    "\n",
    "\n",
    "## Layers, Placeholers and Variables\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "with tf.name_scope(\"Layer1\"):\n",
    "    \n",
    "    W1 = tf.Variable(tf.random_normal([784,256]))\n",
    "    b1 = tf.Variable(tf.random_normal([256]))\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "with tf.name_scope(\"Layer2\"):\n",
    "    \n",
    "    W2 = tf.Variable(tf.random_normal([256,256]))\n",
    "    b2 = tf.Variable(tf.random_normal([256]))\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "    \n",
    "with tf.name_scope(\"Layer3\"):\n",
    "    \n",
    "    W3 = tf.Variable(tf.random_normal([256,10]))\n",
    "    b3 = tf.Variable(tf.random_normal([10]))\n",
    "    hypothesis = tf.matmul(L2,W3) + b3\n",
    "\n",
    "    \n",
    "## Parameters\n",
    "\n",
    "learning_rate = 0.001    \n",
    "batch_size = 100\n",
    "num_epochs = 15\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "                    \n",
    "## Cost/Loss & Optimizer\n",
    "                    \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=hypothesis, labels=tf.stop_gradient(Y)))\n",
    "## tf.stop_gradient? v2 vs v1???\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "                                            \n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, axis=1)) ##tf.argmax 다시 찾아보기\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    \n",
    "## Running the actual session, training the Softmax model\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(logdir)\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost=0\n",
    "        \n",
    "        for iteration in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, summary, cost_val = sess.run([train, merged_summary, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "        \n",
    "        writer.add_summary(summary, global_step=epoch)\n",
    "\n",
    "        print(f\"Epoch: {(epoch +1):04d}, Cost: {avg_cost:.3f}\")\n",
    "        \n",
    "    print(\"Learning is Done!\")\n",
    "                      \n",
    "    print(\"Accuracy:\", sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels},))\n",
    "    \n",
    "    ##get one and predict\n",
    "                      \n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    \n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r: r+1], axis=1)))\n",
    "    print(\"Prediction: \", sess.run(tf.argmax(hypothesis, axis=1), feed_dict={X: mnist.test.images[r:r+1]}),)\n",
    "                      \n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28,28), cmap=\"Greys\", interpolation=\"nearest\",)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST(NN with Xavier Initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, Cost: 0.315\n",
      "Epoch: 0002, Cost: 0.115\n",
      "Epoch: 0003, Cost: 0.075\n",
      "Epoch: 0004, Cost: 0.054\n",
      "Epoch: 0005, Cost: 0.041\n",
      "Epoch: 0006, Cost: 0.031\n",
      "Epoch: 0007, Cost: 0.024\n",
      "Epoch: 0008, Cost: 0.019\n",
      "Epoch: 0009, Cost: 0.017\n",
      "Epoch: 0010, Cost: 0.016\n",
      "Epoch: 0011, Cost: 0.011\n",
      "Epoch: 0012, Cost: 0.011\n",
      "Epoch: 0013, Cost: 0.008\n",
      "Epoch: 0014, Cost: 0.010\n",
      "Epoch: 0015, Cost: 0.010\n",
      "Learning is Done!\n",
      "Accuracy: 0.9793\n",
      "Label:  [0]\n",
      "Prediction:  [0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Log Directoroy Setting\n",
    "\n",
    "now = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "root_logdir = \"MNIST(NN with Xavier)\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "## Resetting Graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "## Random Seed\n",
    "tf.set_random_seed(100)\n",
    "\n",
    "\n",
    "## Layers, Placeholers and Variables\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "with tf.name_scope(\"Layer1\"):\n",
    "    \n",
    "    W1 = tf.get_variable(\"W1\", shape=[784,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.random_normal([256]))\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "with tf.name_scope(\"Layer2\"):\n",
    "    \n",
    "    W2 = tf.get_variable(\"W2\", shape=[256,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.random_normal([256]))\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "    \n",
    "with tf.name_scope(\"Layer3\"):\n",
    "    \n",
    "    W3 = tf.get_variable(\"W3\", shape=[256,10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.Variable(tf.random_normal([10]))\n",
    "    hypothesis = tf.matmul(L2,W3) + b3\n",
    "\n",
    "    \n",
    "## Parameters\n",
    "\n",
    "learning_rate = 0.001    \n",
    "batch_size = 100\n",
    "num_epochs = 15\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "                    \n",
    "## Cost/Loss & Optimizer\n",
    "                    \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=hypothesis, labels=tf.stop_gradient(Y)))\n",
    "## tf.stop_gradient? v2 vs v1???\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "                                            \n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, axis=1)) ##tf.argmax 다시 찾아보기\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    \n",
    "## Running the actual session, training the Softmax model\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(logdir)\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost=0\n",
    "        \n",
    "        for iteration in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, summary, cost_val = sess.run([train, merged_summary, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "        \n",
    "        writer.add_summary(summary, global_step=epoch)\n",
    "\n",
    "        print(f\"Epoch: {(epoch +1):04d}, Cost: {avg_cost:.3f}\")\n",
    "        \n",
    "    print(\"Learning is Done!\")\n",
    "                      \n",
    "    print(\"Accuracy:\", sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels},))\n",
    "    \n",
    "    ##get one and predict\n",
    "                      \n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    \n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r: r+1], axis=1)))\n",
    "    print(\"Prediction: \", sess.run(tf.argmax(hypothesis, axis=1), feed_dict={X: mnist.test.images[r:r+1]}),)\n",
    "                      \n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28,28), cmap=\"Greys\", interpolation=\"nearest\",)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST (NN with Xavier, 5 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, Cost: 0.378\n",
      "Epoch: 0002, Cost: 0.107\n",
      "Epoch: 0003, Cost: 0.082\n",
      "Epoch: 0004, Cost: 0.057\n",
      "Epoch: 0005, Cost: 0.050\n",
      "Epoch: 0006, Cost: 0.041\n",
      "Epoch: 0007, Cost: 0.039\n",
      "Epoch: 0008, Cost: 0.033\n",
      "Epoch: 0009, Cost: 0.028\n",
      "Epoch: 0010, Cost: 0.029\n",
      "Epoch: 0011, Cost: 0.028\n",
      "Epoch: 0012, Cost: 0.017\n",
      "Epoch: 0013, Cost: 0.022\n",
      "Epoch: 0014, Cost: 0.018\n",
      "Epoch: 0015, Cost: 0.016\n",
      "Learning is Done!\n",
      "Accuracy: 0.9803\n",
      "Label:  [0]\n",
      "Prediction:  [0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Log Directoroy Setting\n",
    "\n",
    "now = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "root_logdir = \"MNIST(NN with Xavier, 5 layers)\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "## Resetting Graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "## Random Seed\n",
    "tf.set_random_seed(100)\n",
    "\n",
    "\n",
    "## Layers, Placeholers and Variables\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "with tf.name_scope(\"Layer1\"):\n",
    "    \n",
    "    W1 = tf.get_variable(\"W1\", shape=[784,512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.random_normal([512]))\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "with tf.name_scope(\"Layer2\"):\n",
    "    \n",
    "    W2 = tf.get_variable(\"W2\", shape=[512,512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.random_normal([512]))\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "    \n",
    "with tf.name_scope(\"Layer3\"):\n",
    "    \n",
    "    W3 = tf.get_variable(\"W3\", shape=[512,512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.Variable(tf.random_normal([512]))\n",
    "    L3 = tf.matmul(L2,W3) + b3\n",
    "\n",
    "with tf.name_scope(\"Layer4\"):\n",
    "    \n",
    "    W4 = tf.get_variable(\"W4\", shape=[512,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b4 = tf.Variable(tf.random_normal([256]))\n",
    "    L4 = tf.matmul(L3,W4) + b4\n",
    "\n",
    "with tf.name_scope(\"Layer5\"):\n",
    "    \n",
    "    W5 = tf.get_variable(\"W5\", shape=[256,10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b5 = tf.Variable(tf.random_normal([10]))\n",
    "    hypothesis = tf.matmul(L4,W5) + b5\n",
    "\n",
    "    \n",
    "    \n",
    "## Parameters\n",
    "\n",
    "learning_rate = 0.001    \n",
    "batch_size = 100\n",
    "num_epochs = 15\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "                    \n",
    "## Cost/Loss & Optimizer\n",
    "                    \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=hypothesis, labels=tf.stop_gradient(Y)))\n",
    "## tf.stop_gradient? v2 vs v1???\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "                                            \n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, axis=1)) ##tf.argmax 다시 찾아보기\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    \n",
    "## Running the actual session, training the Softmax model\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(logdir)\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost=0\n",
    "        \n",
    "        for iteration in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, summary, cost_val = sess.run([train, merged_summary, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "        \n",
    "        writer.add_summary(summary, global_step=epoch)\n",
    "\n",
    "        print(f\"Epoch: {(epoch +1):04d}, Cost: {avg_cost:.3f}\")\n",
    "        \n",
    "    print(\"Learning is Done!\")\n",
    "                      \n",
    "    print(\"Accuracy:\", sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels},))\n",
    "    \n",
    "    ##get one and predict\n",
    "                      \n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    \n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r: r+1], axis=1)))\n",
    "    print(\"Prediction: \", sess.run(tf.argmax(hypothesis, axis=1), feed_dict={X: mnist.test.images[r:r+1]}),)\n",
    "                      \n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28,28), cmap=\"Greys\", interpolation=\"nearest\",)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST (NN with Xavier, 5 layers, Dropouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, Cost: 0.686\n",
      "Epoch: 0002, Cost: 0.255\n",
      "Epoch: 0003, Cost: 0.193\n",
      "Epoch: 0004, Cost: 0.161\n",
      "Epoch: 0005, Cost: 0.140\n",
      "Epoch: 0006, Cost: 0.126\n",
      "Epoch: 0007, Cost: 0.123\n",
      "Epoch: 0008, Cost: 0.112\n",
      "Epoch: 0009, Cost: 0.110\n",
      "Epoch: 0010, Cost: 0.100\n",
      "Epoch: 0011, Cost: 0.099\n",
      "Epoch: 0012, Cost: 0.099\n",
      "Epoch: 0013, Cost: 0.090\n",
      "Epoch: 0014, Cost: 0.089\n",
      "Epoch: 0015, Cost: 0.086\n",
      "Learning is Done!\n",
      "Accuracy: 0.981\n",
      "Label:  [1]\n",
      "Prediction:  [1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Log Directoroy Setting\n",
    "\n",
    "now = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "root_logdir = \"MNIST(NN with Dropouts)\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "## Resetting Graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "## Random Seed\n",
    "tf.set_random_seed(100)\n",
    "\n",
    "\n",
    "## Layers, Placeholers and Variables\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "\n",
    "## Dropout rate (0.5 for training, 1 for testing)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "## NN Layers\n",
    "\n",
    "with tf.name_scope(\"Layer1\"):\n",
    "    \n",
    "    W1 = tf.get_variable(\"W1\", shape=[784,512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.random_normal([512]))\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    L1 = tf.nn.dropout(L1, keep_prob = keep_prob)\n",
    "    \n",
    "    \n",
    "with tf.name_scope(\"Layer2\"):\n",
    "    \n",
    "    W2 = tf.get_variable(\"W2\", shape=[512,512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.random_normal([512]))\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "    L1 = tf.nn.dropout(L2, keep_prob = keep_prob)\n",
    "    \n",
    "with tf.name_scope(\"Layer3\"):\n",
    "    \n",
    "    W3 = tf.get_variable(\"W3\", shape=[512,512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.Variable(tf.random_normal([512]))\n",
    "    L3 = tf.matmul(L2,W3) + b3\n",
    "    L3 = tf.nn.dropout(L3, keep_prob = keep_prob)\n",
    "\n",
    "with tf.name_scope(\"Layer4\"):\n",
    "    \n",
    "    W4 = tf.get_variable(\"W4\", shape=[512,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b4 = tf.Variable(tf.random_normal([256]))\n",
    "    L4 = tf.matmul(L3,W4) + b4\n",
    "    L4 = tf.nn.dropout(L4, keep_prob = keep_prob)\n",
    "    \n",
    "with tf.name_scope(\"Layer5\"):\n",
    "    \n",
    "    W5 = tf.get_variable(\"W5\", shape=[256,10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b5 = tf.Variable(tf.random_normal([10]))\n",
    "    hypothesis = tf.matmul(L4,W5) + b5\n",
    "        \n",
    "        \n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 15\n",
    "batch_size = 100\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)        \n",
    "                    \n",
    "    \n",
    "## Cost/Loss & Optimizer\n",
    "                    \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=hypothesis, labels=tf.stop_gradient(Y)))\n",
    "## tf.stop_gradient? v2 vs v1???\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "                                            \n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, axis=1)) ##tf.argmax 다시 찾아보기\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    \n",
    "## Running the actual session, training the Softmax model\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(logdir)\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost=0\n",
    "        \n",
    "        for iteration in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, summary, cost_val = sess.run([train, merged_summary, cost], feed_dict={X: batch_xs, Y: batch_ys, keep_prob: 0.5})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "        \n",
    "        writer.add_summary(summary, global_step=epoch)\n",
    "\n",
    "        print(f\"Epoch: {(epoch +1):04d}, Cost: {avg_cost:.3f}\")\n",
    "        \n",
    "    print(\"Learning is Done!\")\n",
    "                      \n",
    "    print(\"Accuracy:\", sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1},))\n",
    "    \n",
    "    ##get one and predict\n",
    "                      \n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    \n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r: r+1], axis=1)))\n",
    "    print(\"Prediction: \", sess.run(tf.argmax(hypothesis, axis=1), feed_dict={X: mnist.test.images[r:r+1], keep_prob: 1}))\n",
    "                      \n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28,28), cmap=\"Greys\", interpolation=\"nearest\",)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST (+ Batch Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
